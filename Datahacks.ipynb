{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmzhang7/youtube_comments/blob/main/Datahacks_another.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXjCc7mQZSg6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaFdYCidXbm4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6nvl9NtXiP4"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/youtube')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgdwBkcoYohJ"
      },
      "outputs": [],
      "source": [
        "video_df = pd.read_csv('USvideos.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYD4pwSKY49W"
      },
      "outputs": [],
      "source": [
        "video_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qShuc0MZbY2F"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvBW6dxwahYN"
      },
      "outputs": [],
      "source": [
        "comments_df = pd.read_csv('UScomments 2.csv', usecols=['video_id', 'comment_text', 'likes', 'replies'],encoding_errors='ignore')\n",
        "comments_df['likes'] = pd.to_numeric(comments_df['likes'], errors='coerce')\n",
        "comments_df['replies'] = pd.to_numeric(comments_df['replies'], errors='coerce')\n",
        "comments_df = comments_df.dropna(subset=['likes', 'replies'])\n",
        "comments_df = comments_df.drop_duplicates(subset=['video_id', 'comment_text'])\n",
        "comments_df['comment_weight'] = comments_df['likes'] + comments_df['replies']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTktTcuggpau"
      },
      "outputs": [],
      "source": [
        "comments_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kVKA5H7bUmD"
      },
      "outputs": [],
      "source": [
        "with open('US_category_id.json', 'r') as file:\n",
        "    data = file.read()\n",
        "    categories = json.loads(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRDpagW54LG1"
      },
      "outputs": [],
      "source": [
        "comments_more_than_one_word = comments_df[comments_df['comment_text'].str.split().str.len() > 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1ffvScBqLII"
      },
      "outputs": [],
      "source": [
        "#merging two dfs on video_id (converted to string)\n",
        "comments_more_than_one_word['video_id'] = comments_more_than_one_word['video_id'].astype(str)\n",
        "video_df['video_id'] = video_df['video_id'].astype(str)\n",
        "merged_df = pd.merge(comments_more_than_one_word, video_df, on = 'video_id', how = 'inner')\n",
        "merged_df = merged_df.drop_duplicates(subset=['video_id', 'comment_text']).drop(columns = ['comment_total', 'views', 'date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-gEWPLJsIvi"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpbM4cRRakCq"
      },
      "outputs": [],
      "source": [
        "# !pip install fasttext\n",
        "!pip install swifter\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "!pip install langid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCsLHIBCtWg6"
      },
      "outputs": [],
      "source": [
        "#filter out non-English comments\n",
        "!pip install langid\n",
        "!pip install swifter\n",
        "import langid\n",
        "import swifter\n",
        "\n",
        "def is_english_fast(text):\n",
        "    if isinstance(text, str) and len(text.strip()) > 0:\n",
        "        lang, _ = langid.classify(text)\n",
        "        return lang == 'en'\n",
        "    return False\n",
        "\n",
        "merged_df['is_english'] = merged_df['comment_text'].swifter.apply(is_english_fast)\n",
        "merged_english = merged_df[merged_df['is_english']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mabb_kKZ4GYd"
      },
      "outputs": [],
      "source": [
        "#merged_english = pd.merged_english.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_dict = {\n",
        "    1: \"Film & Animation\",\n",
        "    2: \"Autos & Vehicles\",\n",
        "    10: \"Music\",\n",
        "    15: \"Pets & Animals\",\n",
        "    17: \"Sports\",\n",
        "    18: \"Short Movies\",\n",
        "    19: \"Travels and Events\",\n",
        "    20: \"Gaming\",\n",
        "    21: \"Videoblogging\",\n",
        "    22: \"People & Blogs\",\n",
        "    23: \"Comedy\",\n",
        "    24: \"Entertainment\",\n",
        "    25: \"News & Politics\",\n",
        "    26: \"Howto & Style\",\n",
        "    27: \"Education\",\n",
        "    28: \"Science & Technology\",\n",
        "    29: \"Nonprofits & Activism\",\n",
        "    30: \"Movies\",\n",
        "    31: \"Anime/Animation\",\n",
        "    32: \"Action/Adventure\",\n",
        "    33: \"Classics\",\n",
        "    34: \"Comedy\",\n",
        "    35: \"Documentary\",\n",
        "    36: \"Drama\",\n",
        "    37: \"Family\",\n",
        "    38: \"Foreign\",\n",
        "    39: \"Horror\",\n",
        "    40: \"Sci-Fi/Fantasy\",\n",
        "    41: \"Thriller\",\n",
        "    42: \"Shorts\",\n",
        "    43: \"Shows\",\n",
        "    44: \"Trailers\"\n",
        "}\n",
        "merged_english['category_name'] = merged_english['category_id'].map(category_dict).drop(columns = ['category_id'])"
      ],
      "metadata": {
        "id": "osRCcBFOR7AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "567oezWZ7vXx"
      },
      "outputs": [],
      "source": [
        "#convert category id to names, keep the top 10 most common\n",
        "category_counts = merged_english['category_name'].value_counts()\n",
        "top_categories = category_counts[category_counts >= 10000].head(10).index\n",
        "merged_english_filtered = merged_english[merged_english['category_name'].isin(top_categories)]\n",
        "merged_english_filtered\n",
        "\n",
        "#clean comment texts like removing /n, links, non-punctuation symbols, etc.\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', '')\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "    #text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"]', '', text)\n",
        "    text = re.sub(r'[^\\w\\s.,!?\\'\\\"üòÉ-üôè‚ù§üß†üî•üò≠üíÄüòÖüòâü§£ü§°ü§†üòéü•≤ü•∫ü§îüëÄ‚≠êüåüüåàüéâ‚ú®‚ù§Ô∏èü´∂ü§≠üò≠‚ù§Ô∏èüòçüëç]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "merged_english_filtered['comment_text'] = merged_english_filtered['comment_text'].apply(clean_text)\n",
        "merged_english_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dljq1ThjuMmV"
      },
      "outputs": [],
      "source": [
        "#clean tags\n",
        "merged_english['clean_tags'] = (\n",
        "    merged_english['tags']\n",
        "    .fillna('')\n",
        "    .str.lower()\n",
        "    .str.split('|')\n",
        "    .apply(lambda x: list(dict.fromkeys([tag.strip() for tag in x if tag.strip()])))\n",
        ")\n",
        "merged_english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl_v7UaW63RG"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "tag_list = [tag for tag_list_row in merged_english['clean_tags'].dropna() for tag in tag_list_row]\n",
        "\n",
        "tag_counts = Counter(tag_list)\n",
        "\n",
        "excluded_tags = {'[none]', '', 'none'}\n",
        "\n",
        "filtered_tag_counts = {tag: count for tag, count in tag_counts.items() if tag.lower() not in excluded_tags}\n",
        "\n",
        "top_10_tags = Counter(filtered_tag_counts).most_common(10)\n",
        "\n",
        "top_10_tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZQuftTXPKhM"
      },
      "outputs": [],
      "source": [
        "merged_english['comment_weight'].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBOkF44jvWt7"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sojcGTohvdgH"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zSx6DfXvoTo"
      },
      "outputs": [],
      "source": [
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pSdLBdYwK6r"
      },
      "outputs": [],
      "source": [
        "st.title(\"Random Comment Generator with Models\")\n",
        "\n",
        "# Selectbox to choose a model\n",
        "model_choice = st.selectbox(\"Choose a model\", [\"Markov Model 1\", \"Markov Model 2\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iPFv3YDGApP"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzEhkjE7DA5A"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxrbPiURMZA5"
      },
      "outputs": [],
      "source": [
        "tfidf_small = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TUiszCcKuzM"
      },
      "outputs": [],
      "source": [
        "base_model_small = Pipeline([\n",
        "    ('tfidf', tfidf_small),\n",
        "    ('classifier', LogisticRegression(max_iter = 2000, class_weight = 'balanced'))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf_1fI3BMtRP"
      },
      "outputs": [],
      "source": [
        "weighted_model_small = Pipeline([\n",
        "    ('tfidf', tfidf_small),\n",
        "    ('classifier', LogisticRegression(max_iter = 2000))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpLZs0rWMffz"
      },
      "outputs": [],
      "source": [
        "X = merged_english['comment_text']\n",
        "y = merged_english['category_name']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhEtDNM1NN25"
      },
      "outputs": [],
      "source": [
        "base_model_small.fit(X_train, y_train)\n",
        "y_pred_base = base_model_small.predict(X_test)\n",
        "print(\"=== Baseline Model (Unweighted TF-IDF) ===\")\n",
        "print(classification_report(y_test, y_pred_base))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZZXvXfPQbYR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_Pt4fMNNPgt"
      },
      "outputs": [],
      "source": [
        "weighted_model_small.fit(X_train, y_train, classifier__sample_weight=merged_english.loc[X_train.index, 'comment_weight'])\n",
        "y_pred_weighted_base = weighted_model_small.predict(X_test)\n",
        "print(\"\\n=== Weighted Model ===\")\n",
        "print(classification_report(y_test, y_pred_weighted_base))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4ysytbcKuk7"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    '5K': Pipeline([('tfidf', TfidfVectorizer(max_features=5000)), ('classifier', LogisticRegression(max_iter = 2000, class_weight = 'balanced'))]),\n",
        "    '10K': Pipeline([('tfidf', TfidfVectorizer(max_features=10000)), ('classifier', LogisticRegression(max_iter = 2000, class_weight = 'balanced'))]),\n",
        "    '20K': Pipeline([('tfidf', TfidfVectorizer(max_features=20000)), ('classifier', LogisticRegression(max_iter = 2000, class_weight = 'balanced'))])\n",
        "}\n",
        "\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_test, y_test)\n",
        "    print(f\"{name} features: Accuracy = {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TFvLLnJuFC1"
      },
      "source": [
        "## Markov Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyhMgM8put-t"
      },
      "outputs": [],
      "source": [
        "!pip install markovify\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk4umRk4XX8C"
      },
      "outputs": [],
      "source": [
        "import markovify\n",
        "import numpy as np\n",
        "from transformers import pipeline, set_seed\n",
        "music_comments = merged_english_filtered[merged_english_filtered['category_name'] == 'Music']['comment_text']\n",
        "text_model = markovify.Text(' '.join(music_comments), state_size=4)\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "\n",
        "prompt = \"Category: Music | Mood: Hype | Comment:\"\n",
        "generator(prompt, max_length=50, num_return_sequences=5)\n",
        "\n",
        "#keep comments with most 10 common tags\n",
        "top_10_tags_set = set([tag for tag, _ in top_10_tags])\n",
        "merged_english_filtered_on_both = merged_english_filtered[\n",
        "    merged_english_filtered['clean_tags'].apply(lambda tag_list: any(tag in top_10_tags_set for tag in tag_list))\n",
        "]\n",
        "merged_english_filtered_on_both\n",
        "\n",
        "#GPT-2 Transformer Generator using both category and tags\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "\n",
        "def generate_youtube_comments(category, tags, mood=\"Hype\", num_comments=5):\n",
        "    prompt = f\"Category: {category} | Tags: {tags} | Mood: {mood} | Comment:\"\n",
        "\n",
        "    outputs = generator(\n",
        "        prompt,\n",
        "        max_length=50,\n",
        "        num_return_sequences=num_comments,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        temperature=0.9\n",
        "    )\n",
        "\n",
        "    return [out['generated_text'] for out in outputs]\n",
        "\n",
        "# Test example\n",
        "category = \"Music\"\n",
        "tags = \"funny, vlog, humor\"\n",
        "mood = \"Hype\"\n",
        "\n",
        "print(\"üé§ GPT-2 YouTube Comments for Music + Funny Tags:\")\n",
        "comments = generate_youtube_comments(category, tags, mood)\n",
        "\n",
        "for i, comment in enumerate(comments, 1):\n",
        "    print(f\"{i}. {comment}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7TMsfhK75Uf"
      },
      "source": [
        "## Trigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlxoesLY5ytW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "comments = merged_english_filtered.copy()\n",
        "comments = comments['comment_text'].dropna().tolist()\n",
        "\n",
        "tokens = []\n",
        "for comment in comments:\n",
        "    tokens.extend(word_tokenize(comment.lower()))\n",
        "Trigrams = list(ngrams(tokens, 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate with trigram\n",
        "import collections\n",
        "from collections import defaultdict, Counter\n",
        "model = defaultdict(Counter)\n",
        "\n",
        "for w1, w2, w3 in Trigrams:\n",
        "    model[(w1, w2)][w3] += 1\n",
        "\n",
        "def generate_comment(model, seed=(\"this\", \"video\"), max_len=20):\n",
        "    output = list(seed)\n",
        "    for _ in range(max_len):\n",
        "        next_words = model.get((output[-2], output[-1]), None)\n",
        "        if not next_words:\n",
        "            break\n",
        "        next_word = random.choices(list(next_words.keys()), weights=next_words.values())[0]\n",
        "        output.append(next_word)\n",
        "    return \" \".join(output)"
      ],
      "metadata": {
        "id": "EesNHJJySKBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print(generate_comment(model, seed=(\"i\", \"love\")))\n",
        "print(generate_comment(model, seed=(\"this\", \"video\")))"
      ],
      "metadata": {
        "id": "wmZRq1cFSahU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkkbSmtRT3da"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
